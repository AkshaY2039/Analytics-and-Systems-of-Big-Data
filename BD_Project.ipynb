{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BD Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KW-HrLx6Urc",
        "colab_type": "text"
      },
      "source": [
        "## Addiction patterns Analysis using Decision Tree Classifier, Random Forest and Logistic Regression\n",
        "\n",
        "<br>\n",
        "Python is great for data science modelling, thanks to its numerous modules and packages that help achieve data science goals. But what if the data you are dealing with cannot be fit into a single machine? Maybe you can implement careful sampling to do your analysis on a single machine, but with distributed computing framework like Pyspark, you can efficiently implement the task for large data sets.\n",
        "<br>\n",
        "<br>\n",
        "    \n",
        "Spark API is available in multiple programming languages (Scala, Java, Python and R). There are debates about how Spark performance varies depending on which language you run it on, but since we are comfortable with python, we went ahead with Pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezFVix2q_WZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRrbmTLU_Zjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
        "from pyspark.context import SparkContext\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICBCLiRC7S3_",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "First step in any Apache programming is to create a SparkContext. SparkContext is needed when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. It is first step to connect with Apache Cluster.\n",
        "\n",
        "We have just created an Apache spark context\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W-fWH_vAZLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('daily_smoking_times.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvO0bOfa7x7o",
        "colab_type": "text"
      },
      "source": [
        "#### We import the libraries required here in the below snippet. We shall explore what every function does later below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG5o-SOX_yDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpMNx5kQ8Dhm",
        "colab_type": "text"
      },
      "source": [
        "This is a quick visual of how the data schema looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMeWqbo-Aw6v",
        "colab_type": "code",
        "outputId": "8859456e-f34d-4998-d22d-dba376c61b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "display(df)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Gender: string, Age: int, Time1: int, Time2: int, Time3: int, Time4: int, Time5: int, Time6: int, Time7: int]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Gender='F', Age=13, Time1=4, Time2=8, Time3=13, Time4=14, Time5=19, Time6=22, Time7=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL3yknivA3Et",
        "colab_type": "code",
        "outputId": "77800f0f-5542-4f9e-f634-bf3338beabd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Time1: integer (nullable = true)\n",
            " |-- Time2: integer (nullable = true)\n",
            " |-- Time3: integer (nullable = true)\n",
            " |-- Time4: integer (nullable = true)\n",
            " |-- Time5: integer (nullable = true)\n",
            " |-- Time6: integer (nullable = true)\n",
            " |-- Time7: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgMELTy68mx1",
        "colab_type": "text"
      },
      "source": [
        "#### We label the column Age as a label using the alias function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GXdEYfrGiwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzTQm6y3Gp2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df.select(\"Gender\",\"Time1\",\"Time2\",\"Time3\",\"Time4\",\"Time5\",\"Time6\",\"Time7\", col(\"Age\").alias(\"label\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPxhA5T5G8B1",
        "colab_type": "code",
        "outputId": "802cb10b-dc09-4d1f-95a2-4530d7cde777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "df2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Gender: string, Time1: int, Time2: int, Time3: int, Time4: int, Time5: int, Time6: int, Time7: int, label: int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8AOJhfF8xsU",
        "colab_type": "text"
      },
      "source": [
        "**Here we split the data into 70% and 30% where training is done using the former and testing using the latter.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3c0zlBMG-_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = df2.randomSplit([0.7, 0.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZP9gw_9HJH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = splits[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyTP6xlCHL59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
        "train_rows = train.count()\n",
        "test_rows = test.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h27ToskIHOXa",
        "colab_type": "code",
        "outputId": "c81bed65-3aa5-41f0-d1a9-6560d9daa4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)\n",
        "train.show(5)\n",
        "test.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Rows: 4221  Testing Rows: 1779\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|Gender|Time1|Time2|Time3|Time4|Time5|Time6|Time7|label|\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|     F|    4|    8|   12|   14|   16|   20|    0|   19|\n",
            "|     F|    4|    8|   12|   14|   16|   20|    3|   10|\n",
            "|     F|    4|    8|   12|   14|   16|   21|    0|   41|\n",
            "|     F|    4|    8|   12|   14|   16|   21|    2|   41|\n",
            "|     F|    4|    8|   12|   14|   16|   22|    2|   36|\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+---------+\n",
            "|Gender|Time1|Time2|Time3|Time4|Time5|Time6|Time7|trueLabel|\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+---------+\n",
            "|     F|    4|    8|   12|   14|   16|   20|    3|       37|\n",
            "|     F|    4|    8|   12|   14|   16|   21|    0|       12|\n",
            "|     F|    4|    8|   12|   14|   16|   22|    1|       23|\n",
            "|     F|    4|    8|   12|   14|   16|   23|    2|       40|\n",
            "|     F|    4|    8|   12|   14|   16|   23|    2|       41|\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU62d8XxHikV",
        "colab_type": "code",
        "outputId": "c355af37-01b9-4423-fdaf-da551307c170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "test.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Time1: integer (nullable = true)\n",
            " |-- Time2: integer (nullable = true)\n",
            " |-- Time3: integer (nullable = true)\n",
            " |-- Time4: integer (nullable = true)\n",
            " |-- Time5: integer (nullable = true)\n",
            " |-- Time6: integer (nullable = true)\n",
            " |-- Time7: integer (nullable = true)\n",
            " |-- trueLabel: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9YxDJWj9LKF",
        "colab_type": "text"
      },
      "source": [
        "Here we use StringIndexer to encode a string of labels to a column of label indices.The unseen labels will be put at a  particular index which is specified if user chooses to keep them. If the input column is numeric, we cast it to string and index the string values. \n",
        "\n",
        "When downstream pipeline components such as Estimator or Transformer make use of this string-indexed label, you must set the input column of the component to this string-indexed column name. In many cases, you can set the input column with setInputCol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ObtmwSxHmoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strIdx = StringIndexer(inputCol = \"label\", outputCol = \"typeAge\")\n",
        "labelIdx = StringIndexer(inputCol = \"label\", outputCol = \"idxLabel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPK91MwD-hAU",
        "colab_type": "text"
      },
      "source": [
        "Another utility we use here is the VectorAssembler. \n",
        "\n",
        "**VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2TNFFz7Ho_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "catVect = VectorAssembler(inputCols = [\"typeAge\"], outputCol=\"ageFeatures\")\n",
        "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxAgeFeatures\")\n",
        "numVect = VectorAssembler(inputCols = [\"Gender\",\"Time1\",\"Time2\",\"Time3\",\"Time4\",\"Time5\",\"Time6\",\"Time7\"], outputCol=\"numFeatures\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp-7ZRZS--vZ",
        "colab_type": "text"
      },
      "source": [
        "We also use another utility called **MinMaxScaler** here.\n",
        "\n",
        "![Imgur](https://i.imgur.com/XmvrMK8.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vmlwq9WH1El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
        "featVect = VectorAssembler(inputCols=[\"idxAgeFeatures\", \"normFeatures\"], outputCol=\"features\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPn0II-RH3sL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl = []\n",
        "pipeline = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGXO2uomCopZ",
        "colab_type": "text"
      },
      "source": [
        "We build 3 classifiers and then insert them into a pipeline.\n",
        "\n",
        "## **What is a Pipeline?**\n",
        "\n",
        "In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
        "\n",
        "    Split each document’s text into words.\n",
        "    Convert each document’s words into a numerical feature vector.\n",
        "    Learn a prediction model using the feature vectors and labels.\n",
        "\n",
        "MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. We will use this simple workflow as a running example in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqbO3JxOC3XA",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://spark.apache.org/docs/latest/img/ml-Pipeline.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjrUjDLYH6B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl.insert(0, DecisionTreeClassifier(labelCol=\"idxLabel\", featuresCol=\"features\"))\n",
        "cl.insert(1, RandomForestClassifier(labelCol=\"idxLabel\", featuresCol=\"features\"))\n",
        "cl.insert(2, LogisticRegression(labelCol=\"idxLabel\", featuresCol=\"features\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa4hMa42DFKN",
        "colab_type": "text"
      },
      "source": [
        "**Decision trees** are a popular family of classification and regression methods. More information about the spark.ml implementation can be found further in the section on decision trees.\n",
        "<br>\n",
        "\n",
        "**Random Forests**\n",
        "\n",
        "Random forests are ensembles of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF1kfxXXHsrK",
        "colab_type": "code",
        "outputId": "c230911e-432a-4efc-d577-6f857e11d383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "for i in range(3):\n",
        "    pipeline.insert(i, Pipeline(stages=[strIdx, labelIdx, catVect, catIdx, numVect, minMax, featVect, cl[i]]))\n",
        "    #piplineModel = Pipeline.fit(train)\n",
        "print(\"Pipeline complete!\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koS-B8O5IIY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = []\n",
        "paramGrid = (ParamGridBuilder().addGrid(cl[0].impurity, (\"gini\", \"entropy\")).addGrid(cl[0].maxDepth, [5, 10, 20]).addGrid(cl[0].maxBins, [5, 10, 20]).build())\n",
        "cv = CrossValidator(estimator=pipeline[0], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=5)\n",
        "\n",
        "model.insert(0, cv.fit(train))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiHk791zMCzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paramGrid2 = (ParamGridBuilder().addGrid(cl[1].impurity, (\"gini\", \"entropy\")).addGrid(cl[1].maxDepth, [5, 10, 20]).addGrid(cl[1].maxBins, [5, 10, 20]).build())\n",
        "cv2 = CrossValidator(estimator=pipeline[1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid2, numFolds=5)\n",
        "\n",
        "model.insert(1, cv2.fit(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q425-a3yMUmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paramGrid = (ParamGridBuilder().addGrid(cl[2].impurity, (\"gini\", \"entropy\")).addGrid(cl[2].maxDepth, [5, 10, 20]).addGrid(cl[2].maxBins, [5, 10, 20]).build())\n",
        "cv = CrossValidator(estimator=pipeline[2], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=5)\n",
        "\n",
        "model.insert(2, cv3.fit(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBDP1GTnMfg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = [] \n",
        "predicted = []\n",
        "for i in range(3):\n",
        "  prediction.insert(i, model[i].transform(test))\n",
        "  predicted.insert(i, prediction[i].select(\"features\", \"prediction\", \"probability\", \"trueLabel\"))\n",
        "  predicted[i].show(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VohRTWfMke2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"trueLabel\", rawPredictionCol=\"prediction\")\n",
        "for i in range(3):\n",
        "    #evaluator = MulticlassClassificationEvaluator(\n",
        "    #labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "    areUPR = evaluator.evaluate(predicted[i], {evaluator.metricName: \"areaUnderPR\"})\n",
        "    areUROC = evaluator.evaluate(predicted[i], {evaluator.metricName: \"areaUnderROC\"})\n",
        "    print(\"AreaUnderPR = %g \" % (areUPR))\n",
        "    \n",
        "    print(\"AreaUnderROC = %g \" % (areUROC))\n",
        "\n",
        "    tp = float(predicted[i].filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
        "    fp = float(predicted[i].filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
        "    tn = float(predicted[i].filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
        "    fn = float(predicted[i].filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    print(\"Precision = %g \" % (precision))\n",
        "    print(\"Recall = %g \" % (recall))\n",
        "\n",
        "    metrics = sqlContext.createDataFrame([\n",
        "    (\"TP\", tp),\n",
        "    (\"FP\", fp),\n",
        "    (\"TN\", tn),\n",
        "    (\"FN\", fn),\n",
        "    (\"Precision\", tp / (tp + fp)),\n",
        "    (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n",
        "    metrics.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}